#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
‰ΩøÁî®Â∑≤ËÆ≠ÁªÉÁöÑÊ®°ÂûãÁîüÊàêÈ¢ÑÊµãÁªìÊûúÔºå‰øùÂ≠ò‰∏∫CSVÊ†ºÂºè
"""
import os
import numpy as np
import pandas as pd
import torch.nn as nn
import torch
import scipy.io as scio
import random
import model
from torch.nn.utils import weight_norm

class TemporalConvNet(nn.Module):
    def __init__(self, num_inputs, num_channels, mode='pre', kernel_size=2, dropout=0.2):
        super(TemporalConvNet, self).__init__()
        layers = []
        num_levels = len(num_channels)
        for i in range(num_levels):
            dilation_size = 2 ** i
            in_channels = num_inputs if i == 0 else num_channels[i-1]
            out_channels = num_channels[i]
            layers += [model.TemporalBlock_v2(in_channels, out_channels, kernel_size,  stride=1, dilation=dilation_size,
                                     padding=(kernel_size-1) * dilation_size, dropout=dropout, mode=mode)]
        self.network = nn.Sequential(*layers)
    def forward(self, x):
        return self.network(x)

def seed_torch(seed=1029):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

## Define forecasting model
class model_fore(nn.Module):
    def __init__(self,input_channel_fore, output_channel_fore, mode, output_size_baselearner=1,
                 kernel_size=2, dropout=0.3, emb_dropout=0.1):
        super().__init__()
        self.mode = mode
        self.tcn = TemporalConvNet(input_channel_fore, output_channel_fore, mode, kernel_size, dropout)
        self.emb_dropout = emb_dropout
        self.drop = nn.Dropout(emb_dropout)
        self.fore_baselearner = nn.Linear(output_channel_fore[-1], output_size_baselearner)
        self.init_weights()
    def init_weights(self):
        self.fore_baselearner.bias.data.fill_(0)
        self.fore_baselearner.weight.data.normal_(0, 0.01)
    def get_trainable_params(self):
        if self.mode == 'pre':
            return self.parameters()
        else:
            trainable_params = []
            for module in self.tcn.modules():
                if hasattr(module, 'get_trainable_params'):
                    trainable_params.extend(module.get_trainable_params())
            trainable_params.extend(self.fore_baselearner.parameters())
            return trainable_params
    def forward(self, x):
        y = self.drop(x)
        y = self.tcn(y.transpose(1, 2)).transpose(1, 2)
        y = self.fore_baselearner(y)
        return y.contiguous()

print("="*70)
print("Âä†ËΩΩÊï∞ÊçÆÂíåÊ®°Âûã...")
print("="*70)

## Load data
seed_torch(seed=1029)
dataFile = 'wf_4_train'
wf_1  = scio.loadmat(dataFile)
p=wf_1['p_1h']
nwp=wf_1['nwp_1h']

P_load1=p[:,0]
P_load=P_load1.reshape(np.size(P_load1,axis=0),-1)
P_nwp1=nwp
nwp_index=[0,1,2,3,4]
for i in range(5):
    if i==0:
        P_nwp=P_nwp1[:,nwp_index[i]].reshape(np.size(P_nwp1,axis=0),-1)
    else:
        P_nwp=np.concatenate((P_nwp,P_nwp1[:,nwp_index[i]].reshape(np.size(P_nwp1,axis=0),-1)),axis=1)

# Define Parameters
dem_realp=1
len_realp=12
Cap=400.5
m=365
d=24
ooo=365
Series_day = P_load.reshape(-1,dem_realp)/Cap
nwp_day = (P_nwp/np.max(abs(P_nwp),axis=0)).reshape(-1,dem_realp*np.size(P_nwp,axis=1))
dem_realc=np.size(P_nwp,axis=1)

# Prepare test data
test_target_p_=Series_day[m*d//dem_realp:(m*d+ooo*d)//dem_realp,:]
test_target_p=test_target_p_.reshape(-1,len_realp,dem_realp)
test_input_c_=nwp_day[m*d//dem_realp:(m*d+ooo*d)//dem_realp,:]
test_input_c=test_input_c_.reshape(-1,len_realp,dem_realc)

Test_target_p=torch.tensor(test_target_p,dtype=torch.float32)
Test_input_c=torch.tensor(test_input_c,dtype=torch.float32)

# Prepare training data
nwp_conven_00=wf_1['nwp_conven_']
p_conven_00=wf_1['p_conven']
p_conven_=p_conven_00/Cap
nwp_conven_=np.empty([1,5],dtype=object)
for i in range(np.size(nwp_conven_00,axis=1)):
    nwp_conven_[0,i]=nwp_conven_00[:,i].reshape(-1,1)/np.max(abs(P_nwp[:,i]),axis=0)
for i_nwp in range(np.size(nwp_conven_, axis=1)):
    if i_nwp == 0:
        nwp_conven_1 = nwp_conven_[0, i_nwp].transpose(1, 0)
        nwp_conven_1 = nwp_conven_1[:, :, np.newaxis]
    else:
        nwp_conven_0 = nwp_conven_[0, i_nwp].transpose(1, 0)
        nwp_conven_1 = np.concatenate((nwp_conven_1, nwp_conven_0[:, :, np.newaxis]), axis=2)
p_conven_1 = p_conven_.transpose(1, 0)
p_conven_1 = p_conven_1[:, :, np.newaxis]
train_target_p = p_conven_1
Train_target_p=torch.tensor(train_target_p,dtype=torch.float32)
Train_input_c=torch.tensor(nwp_conven_1,dtype=torch.float32)

# Define device
device=torch.device("cuda" if torch.cuda.is_available() else "cpu")
device0=torch.device("cpu")
print(f"‰ΩøÁî®ËÆæÂ§á: {device}")

# Create model
model_fore_test_task_query = model_fore(input_channel_fore=dem_realc, output_channel_fore=[128, 96, 64, 48, 32, 16, 8],mode='test_task_support')
model_fore_test_task_query = model_fore_test_task_query.to(device)

print("\nÁîüÊàêÈ¢ÑÊµãÁªìÊûú...")

# Dictionary to store predictions
predictions = {}

# Generate predictions from different models
model_names = [
    "Extreme_Weather_Model_1",
    "Extreme_Weather_Model_2", 
    "Extreme_Weather_Model_3",
    "Extreme_Weather_Model_4",
    "Meta_Learning_Model",
    "Pre_Training_Model"
]

for i_model in range(6):
    with torch.no_grad():
        if i_model<4:
            print(f"  Âä†ËΩΩÊ®°Âûã: model_fore_test_task_support_{i_model}.pth")
            model_fore_test_task_query.load_state_dict(torch.load(f"model_fore_test_task_support_{i_model}.pth"))
        elif i_model==4:
            print(f"  Âä†ËΩΩÊ®°Âûã: model_fore_train_task_query.pth")
            model_fore_test_task_query.load_state_dict(torch.load("model_fore_train_task_query.pth"))
        elif i_model==5:
            print(f"  Âä†ËΩΩÊ®°Âûã: model_fore_pre.pth")
            model_fore_test_task_query.load_state_dict(torch.load("model_fore_pre.pth"))
        
        Test_input_c_device = Test_input_c.to(device)
        Test_output_query=model_fore_test_task_query(Test_input_c_device)
        test_outputs_query=Test_output_query.to(device0)
        test_outputs_query_=np.array(test_outputs_query.reshape(-1,dem_realp))
        
        # Store predictions (restore original scale)
        predictions[model_names[i_model]] = (test_outputs_query_.flatten() * Cap).tolist()

# Get training predictions
print("\n  ÁîüÊàêËÆ≠ÁªÉÈõÜÈ¢ÑÊµã...")
model_fore_pre_temp = model_fore(input_channel_fore=dem_realc, output_channel_fore=[128, 96, 64, 48, 32, 16, 8],mode='pre')
model_fore_pre_temp = model_fore_pre_temp.to(device)
model_fore_pre_temp.load_state_dict(torch.load("model_fore_pre.pth"))
with torch.no_grad():
    Train_input_c_device = Train_input_c.to(device)
    train_outputs_pre = model_fore_pre_temp(Train_input_c_device).to(device0)

train_outputs_pre_=np.array(train_outputs_pre.reshape(-1,dem_realp))
train_target_p_=train_target_p.reshape(-1,dem_realp)
test_target_p_=test_target_p.reshape(-1,dem_realp)

# Create test results DataFrame
print("\nÂàõÂª∫CSVÊñá‰ª∂...")
test_df = pd.DataFrame()
test_df['Time_Index'] = range(len(test_target_p_.flatten()))
test_df['True_Power_MW'] = (test_target_p_.flatten() * Cap).round(4)

for model_name in model_names:
    test_df[f'Pred_{model_name}_MW'] = [round(x, 4) for x in predictions[model_name]]

# Calculate errors
for model_name in model_names:
    test_df[f'Error_{model_name}_MW'] = (test_df[f'Pred_{model_name}_MW'] - test_df['True_Power_MW']).round(4)

# Create training results DataFrame
train_df = pd.DataFrame()
train_df['Time_Index'] = range(len(train_target_p_.flatten()))
train_df['True_Power_MW'] = (train_target_p_.flatten() * Cap).round(4)
train_df['Pred_Pre_Training_Model_MW'] = (train_outputs_pre_.flatten() * Cap).round(4)
train_df['Error_Pre_Training_Model_MW'] = (train_df['Pred_Pre_Training_Model_MW'] - train_df['True_Power_MW']).round(4)

# Calculate statistics
print("\nËÆ°ÁÆóËØÑ‰º∞ÊåáÊ†á...")
stats_data = []
for model_name in model_names:
    pred_col = f'Pred_{model_name}_MW'
    true_vals = test_df['True_Power_MW'].values
    pred_vals = test_df[pred_col].values
    
    mae = np.mean(np.abs(true_vals - pred_vals))
    rmse = np.sqrt(np.mean((true_vals - pred_vals)**2))
    mape = np.mean(np.abs((true_vals - pred_vals) / (true_vals + 1e-8))) * 100
    r2 = 1 - (np.sum((true_vals - pred_vals)**2) / np.sum((true_vals - np.mean(true_vals))**2))
    
    stats_data.append({
        'Model': model_name,
        'MAE_MW': round(mae, 4),
        'RMSE_MW': round(rmse, 4),
        'MAPE_%': round(mape, 4),
        'R2_Score': round(r2, 4)
    })

stats_df = pd.DataFrame(stats_data)

# Save to CSV
print("\n‰øùÂ≠òCSVÊñá‰ª∂...")
test_df.to_csv('test_predictions.csv', index=False, encoding='utf-8-sig')
train_df.to_csv('train_predictions.csv', index=False, encoding='utf-8-sig')
stats_df.to_csv('model_performance_metrics.csv', index=False, encoding='utf-8-sig')

print("\n" + "="*70)
print("‚úì‚úì‚úì CSVÊñá‰ª∂Â∑≤ÁîüÊàêÔºÅ")
print("="*70)
print("\nÁîüÊàêÁöÑÊñá‰ª∂:")
print(f"  üìä test_predictions.csv ({len(test_df)} Ë°å)")
print("     - Time_Index: Êó∂Èó¥Á¥¢Âºï")
print("     - True_Power_MW: ÁúüÂÆûÂäüÁéá (MW)")
print("     - Pred_*_MW: ÂêÑÊ®°ÂûãÈ¢ÑÊµãÂäüÁéá (MW)")
print("     - Error_*_MW: È¢ÑÊµãËØØÂ∑Æ (MW)")
print(f"\n  üìä train_predictions.csv ({len(train_df)} Ë°å)")
print("     - Time_Index: Êó∂Èó¥Á¥¢Âºï")
print("     - True_Power_MW: ÁúüÂÆûÂäüÁéá (MW)")
print("     - Pred_Pre_Training_Model_MW: È¢ÑËÆ≠ÁªÉÊ®°ÂûãÈ¢ÑÊµã (MW)")
print("     - Error_Pre_Training_Model_MW: È¢ÑÊµãËØØÂ∑Æ (MW)")
print(f"\n  üìà model_performance_metrics.csv (ËØÑ‰º∞ÊåáÊ†á)")
print("     - MAE: Âπ≥ÂùáÁªùÂØπËØØÂ∑Æ")
print("     - RMSE: ÂùáÊñπÊ†πËØØÂ∑Æ")
print("     - MAPE: Âπ≥ÂùáÁªùÂØπÁôæÂàÜÊØîËØØÂ∑Æ")
print("     - R2_Score: ÂÜ≥ÂÆöÁ≥ªÊï∞")
print("\nÊï∞ÊçÆËØ¥Êòé:")
print("  - ÊâÄÊúâÂäüÁéáÂÄºÂ∑≤ÊÅ¢Â§çÂà∞ÂéüÂßãÂ∞∫Â∫¶ (MW)")
print("  - Cap = 400.5 MW (È¢ùÂÆöÂÆπÈáè)")
print("  - ÊµãËØïÈõÜ: 365Â§©Êï∞ÊçÆ")
print("  - ËÆ≠ÁªÉÈõÜ: Â∏∏ËßÑÂú∫ÊôØÊï∞ÊçÆ")
print("="*70)

# Display statistics
print("\nÊ®°ÂûãÊÄßËÉΩÂØπÊØî:")
print(stats_df.to_string(index=False))
print("="*70)
