#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨å·²è®­ç»ƒçš„æ¨¡å‹ç”Ÿæˆé¢„æµ‹ç»“æœï¼Œä¿å­˜ä¸ºCSVæ ¼å¼
"""
import os
import numpy as np
import pandas as pd
import torch.nn as nn
import torch
import scipy.io as scio
import random
import model
from torch.nn.utils import weight_norm

class TemporalConvNet(nn.Module):
    def __init__(self, num_inputs, num_channels, mode='pre', kernel_size=2, dropout=0.2):
        super(TemporalConvNet, self).__init__()
        layers = []
        num_levels = len(num_channels)
        for i in range(num_levels):
            dilation_size = 2 ** i
            in_channels = num_inputs if i == 0 else num_channels[i-1]
            out_channels = num_channels[i]
            layers += [model.TemporalBlock_v2(in_channels, out_channels, kernel_size,  stride=1, dilation=dilation_size,
                                     padding=(kernel_size-1) * dilation_size, dropout=dropout, mode=mode)]
        self.network = nn.Sequential(*layers)
    def forward(self, x):
        return self.network(x)

def seed_torch(seed=1029):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

## Define forecasting model
class model_fore(nn.Module):
    def __init__(self,input_channel_fore, output_channel_fore, mode, output_size_baselearner=1,
                 kernel_size=2, dropout=0.3, emb_dropout=0.1):
        super().__init__()
        self.mode = mode
        self.tcn = TemporalConvNet(input_channel_fore, output_channel_fore, mode, kernel_size, dropout)
        self.emb_dropout = emb_dropout
        self.drop = nn.Dropout(emb_dropout)
        self.fore_baselearner = nn.Linear(output_channel_fore[-1], output_size_baselearner)
        self.init_weights()
    def init_weights(self):
        self.fore_baselearner.bias.data.fill_(0)
        self.fore_baselearner.weight.data.normal_(0, 0.01)
    def get_trainable_params(self):
        if self.mode == 'pre':
            return self.parameters()
        else:
            trainable_params = []
            for module in self.tcn.modules():
                if hasattr(module, 'get_trainable_params'):
                    trainable_params.extend(module.get_trainable_params())
            trainable_params.extend(self.fore_baselearner.parameters())
            return trainable_params
    def forward(self, x):
        y = self.drop(x)
        y = self.tcn(y.transpose(1, 2)).transpose(1, 2)
        y = self.fore_baselearner(y)
        return y.contiguous()

print("="*70)
print("åŠ è½½æ•°æ®å’Œæ¨¡å‹...")
print("="*70)

## Load data
seed_torch(seed=1029)
dataFile = '58wf_4_train'  # æ›´æ–°åçš„æ•°æ®æ–‡ä»¶ï¼ˆæ ‡å¹ºå€¼ï¼‰
wf_1  = scio.loadmat(dataFile)
p=wf_1['p_1h']
nwp=wf_1['nwp_1h']

P_load1=p[:,0]
P_load=P_load1.reshape(np.size(P_load1,axis=0),-1)
P_nwp1=nwp
nwp_index=[0,1,2,3,4]
for i in range(5):
    if i==0:
        P_nwp=P_nwp1[:,nwp_index[i]].reshape(np.size(P_nwp1,axis=0),-1)
    else:
        P_nwp=np.concatenate((P_nwp,P_nwp1[:,nwp_index[i]].reshape(np.size(P_nwp1,axis=0),-1)),axis=1)

# Define Parameters
dem_realp=1
len_realp=12
Cap=50  # æ€»è£…æœºå®¹é‡ (MW)
m=365
d=24
ooo=365
# æ•°æ®å·²ç»æ˜¯æ ‡å¹ºå€¼ï¼Œä¸éœ€è¦å†å½’ä¸€åŒ–
Series_day = P_load.reshape(-1,dem_realp)
nwp_day = (P_nwp/np.max(abs(P_nwp),axis=0)).reshape(-1,dem_realp*np.size(P_nwp,axis=1))
dem_realc=np.size(P_nwp,axis=1)

# Prepare test data
test_target_p_=Series_day[m*d//dem_realp:(m*d+ooo*d)//dem_realp,:]
test_target_p=test_target_p_.reshape(-1,len_realp,dem_realp)
test_input_c_=nwp_day[m*d//dem_realp:(m*d+ooo*d)//dem_realp,:]
test_input_c=test_input_c_.reshape(-1,len_realp,dem_realc)

Test_target_p=torch.tensor(test_target_p,dtype=torch.float32)
Test_input_c=torch.tensor(test_input_c,dtype=torch.float32)

# Prepare training data
nwp_conven_00=wf_1['nwp_conven_']
p_conven_00=wf_1['p_conven']
p_conven_=p_conven_00  # æ•°æ®å·²ç»æ˜¯æ ‡å¹ºå€¼
nwp_conven_=np.empty([1,5],dtype=object)
for i in range(np.size(nwp_conven_00,axis=1)):
    nwp_conven_[0,i]=nwp_conven_00[:,i].reshape(-1,1)/np.max(abs(P_nwp[:,i]),axis=0)
for i_nwp in range(np.size(nwp_conven_, axis=1)):
    if i_nwp == 0:
        nwp_conven_1 = nwp_conven_[0, i_nwp].transpose(1, 0)
        nwp_conven_1 = nwp_conven_1[:, :, np.newaxis]
    else:
        nwp_conven_0 = nwp_conven_[0, i_nwp].transpose(1, 0)
        nwp_conven_1 = np.concatenate((nwp_conven_1, nwp_conven_0[:, :, np.newaxis]), axis=2)
p_conven_1 = p_conven_.transpose(1, 0)
p_conven_1 = p_conven_1[:, :, np.newaxis]
train_target_p = p_conven_1
Train_target_p=torch.tensor(train_target_p,dtype=torch.float32)
Train_input_c=torch.tensor(nwp_conven_1,dtype=torch.float32)

# Define device
device=torch.device("cuda" if torch.cuda.is_available() else "cpu")
device0=torch.device("cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# Create model
model_fore_test_task_query = model_fore(input_channel_fore=dem_realc, output_channel_fore=[128, 96, 64, 48, 32, 16, 8],mode='test_task_support')
model_fore_test_task_query = model_fore_test_task_query.to(device)

print("\nç”Ÿæˆé¢„æµ‹ç»“æœ...")

# Dictionary to store predictions
predictions = {}

# Generate predictions from different models
model_names = [
    "Extreme_Weather_Model_1",
    "Extreme_Weather_Model_2", 
    "Extreme_Weather_Model_3",
    "Extreme_Weather_Model_4",
    "Meta_Learning_Model",
    "Pre_Training_Model"
]

for i_model in range(6):
    with torch.no_grad():
        if i_model<4:
            print(f"  åŠ è½½æ¨¡å‹: model_fore_test_task_support_{i_model}.pth")
            model_fore_test_task_query.load_state_dict(torch.load(f"model_fore_test_task_support_{i_model}.pth"))
        elif i_model==4:
            # Meta Learning Modelï¼šä¼˜å…ˆä½¿ç”¨train_task_queryï¼Œä¸å­˜åœ¨åˆ™ä½¿ç”¨é¢„è®­ç»ƒ
            import os
            if os.path.exists("model_fore_train_task_query.pth"):
                print(f"  åŠ è½½æ¨¡å‹: model_fore_train_task_query.pthï¼ˆå…ƒè®­ç»ƒæ¨¡å‹ï¼‰")
                model_fore_test_task_query.load_state_dict(torch.load("model_fore_train_task_query.pth"))
            elif os.path.exists("model_fore_pre_federated.pth"):
                print(f"  åŠ è½½æ¨¡å‹: model_fore_pre_federated.pthï¼ˆè·³è¿‡äº†å…ƒè®­ç»ƒï¼‰")
                model_fore_test_task_query.load_state_dict(torch.load("model_fore_pre_federated.pth"))
            else:
                print(f"  åŠ è½½æ¨¡å‹: model_fore_pre.pthï¼ˆè·³è¿‡äº†å…ƒè®­ç»ƒï¼‰")
                model_fore_test_task_query.load_state_dict(torch.load("model_fore_pre.pth"))
        elif i_model==5:
            # Pre-training Modelï¼šä¼˜å…ˆä½¿ç”¨è”é‚¦ç‰ˆ
            import os
            if os.path.exists("model_fore_pre_federated.pth"):
                print(f"  åŠ è½½æ¨¡å‹: model_fore_pre_federated.pthï¼ˆè”é‚¦é¢„è®­ç»ƒï¼‰")
                model_fore_test_task_query.load_state_dict(torch.load("model_fore_pre_federated.pth"))
            else:
                print(f"  åŠ è½½æ¨¡å‹: model_fore_pre.pthï¼ˆå•åœºç«™é¢„è®­ç»ƒï¼‰")
                model_fore_test_task_query.load_state_dict(torch.load("model_fore_pre.pth"))
        
        Test_input_c_device = Test_input_c.to(device)
        Test_output_query=model_fore_test_task_query(Test_input_c_device)
        test_outputs_query=Test_output_query.to(device0)
        test_outputs_query_=np.array(test_outputs_query.reshape(-1,dem_realp))
        
        # Store predictions (keep in per unit, will convert to MW later)
        predictions[model_names[i_model]] = test_outputs_query_.flatten().tolist()

# Get training predictions
print("\n  ç”Ÿæˆè®­ç»ƒé›†é¢„æµ‹...")
model_fore_pre_temp = model_fore(input_channel_fore=dem_realc, output_channel_fore=[128, 96, 64, 48, 32, 16, 8],mode='pre')
model_fore_pre_temp = model_fore_pre_temp.to(device)
model_fore_pre_temp.load_state_dict(torch.load("model_fore_pre.pth"))
with torch.no_grad():
    Train_input_c_device = Train_input_c.to(device)
    train_outputs_pre = model_fore_pre_temp(Train_input_c_device).to(device0)

train_outputs_pre_=np.array(train_outputs_pre.reshape(-1,dem_realp))
train_target_p_=train_target_p.reshape(-1,dem_realp)
test_target_p_=test_target_p.reshape(-1,dem_realp)

# Create test results DataFrame
print("\nåˆ›å»ºCSVæ–‡ä»¶...")
test_df = pd.DataFrame()
test_df['Time_Index'] = range(len(test_target_p_.flatten()))
# æ ‡å¹ºå€¼å’ŒMWå€¼éƒ½ä¿å­˜
test_df['True_Power_pu'] = test_target_p_.flatten().round(6)
test_df['True_Power_MW'] = (test_target_p_.flatten() * Cap).round(4)

for model_name in model_names:
    # æ ‡å¹ºå€¼
    test_df[f'Pred_{model_name}_pu'] = [round(x, 6) for x in predictions[model_name]]
    # MWå€¼
    test_df[f'Pred_{model_name}_MW'] = [round(x * Cap, 4) for x in predictions[model_name]]

# Calculate errors (in per unit)
for model_name in model_names:
    test_df[f'Error_{model_name}_pu'] = (test_df[f'Pred_{model_name}_pu'] - test_df['True_Power_pu']).round(6)
    test_df[f'Error_{model_name}_MW'] = (test_df[f'Pred_{model_name}_MW'] - test_df['True_Power_MW']).round(4)

# Create training results DataFrame
train_df = pd.DataFrame()
train_df['Time_Index'] = range(len(train_target_p_.flatten()))
train_df['True_Power_pu'] = train_target_p_.flatten().round(6)
train_df['True_Power_MW'] = (train_target_p_.flatten() * Cap).round(4)
train_df['Pred_Pre_Training_Model_pu'] = train_outputs_pre_.flatten().round(6)
train_df['Pred_Pre_Training_Model_MW'] = (train_outputs_pre_.flatten() * Cap).round(4)
train_df['Error_Pre_Training_Model_pu'] = (train_df['Pred_Pre_Training_Model_pu'] - train_df['True_Power_pu']).round(6)
train_df['Error_Pre_Training_Model_MW'] = (train_df['Pred_Pre_Training_Model_MW'] - train_df['True_Power_MW']).round(4)

# Calculate statistics (using per unit values, displayed as percentage)
print("\nè®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼ˆç™¾åˆ†æ¯”å½¢å¼ï¼‰...")
stats_data = []
for model_name in model_names:
    # ä½¿ç”¨æ ‡å¹ºå€¼è®¡ç®—
    pred_col_pu = f'Pred_{model_name}_pu'
    true_vals_pu = test_df['True_Power_pu'].values
    pred_vals_pu = test_df[pred_col_pu].values
    
    # æ ‡å¹ºå€¼æŒ‡æ ‡è½¬æ¢ä¸ºç™¾åˆ†æ¯” (Ã—100)
    mae_percent = np.mean(np.abs(true_vals_pu - pred_vals_pu)) * 100
    rmse_percent = np.sqrt(np.mean((true_vals_pu - pred_vals_pu)**2)) * 100
    mape = np.mean(np.abs((true_vals_pu - pred_vals_pu) / (true_vals_pu + 1e-8))) * 100
    r2 = 1 - (np.sum((true_vals_pu - pred_vals_pu)**2) / np.sum((true_vals_pu - np.mean(true_vals_pu))**2))
    
    stats_data.append({
        'Model': model_name,
        'MAE_%': round(mae_percent, 4),
        'RMSE_%': round(rmse_percent, 4),
        'MAPE_%': round(mape, 4),
        'R2_Score': round(r2, 4)
    })

stats_df = pd.DataFrame(stats_data)

# Save to CSV
print("\nä¿å­˜CSVæ–‡ä»¶...")
test_df.to_csv('test_predictions.csv', index=False, encoding='utf-8-sig')
train_df.to_csv('train_predictions.csv', index=False, encoding='utf-8-sig')
stats_df.to_csv('model_performance_metrics.csv', index=False, encoding='utf-8-sig')

print("\n" + "="*70)
print("âœ“âœ“âœ“ CSVæ–‡ä»¶å·²ç”Ÿæˆï¼")
print("="*70)
print("\nç”Ÿæˆçš„æ–‡ä»¶:")
print(f"  ğŸ“Š test_predictions.csv ({len(test_df)} è¡Œ)")
print("     - Time_Index: æ—¶é—´ç´¢å¼•")
print("     - True_Power_MW: çœŸå®åŠŸç‡ (MW)")
print("     - Pred_*_MW: å„æ¨¡å‹é¢„æµ‹åŠŸç‡ (MW)")
print("     - Error_*_MW: é¢„æµ‹è¯¯å·® (MW)")
print(f"\n  ğŸ“Š train_predictions.csv ({len(train_df)} è¡Œ)")
print("     - Time_Index: æ—¶é—´ç´¢å¼•")
print("     - True_Power_MW: çœŸå®åŠŸç‡ (MW)")
print("     - Pred_Pre_Training_Model_MW: é¢„è®­ç»ƒæ¨¡å‹é¢„æµ‹ (MW)")
print("     - Error_Pre_Training_Model_MW: é¢„æµ‹è¯¯å·® (MW)")
print(f"\n  ğŸ“ˆ model_performance_metrics.csv (è¯„ä¼°æŒ‡æ ‡)")
print("     - MAE_%: å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆç™¾åˆ†æ¯”å½¢å¼ï¼‰")
print("     - RMSE_%: å‡æ–¹æ ¹è¯¯å·®ï¼ˆç™¾åˆ†æ¯”å½¢å¼ï¼‰")
print("     - MAPE_%: å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®")
print("     - R2_Score: å†³å®šç³»æ•°")
print("\næ•°æ®è¯´æ˜:")
print("  - åŠŸç‡å€¼åŒæ—¶æä¾›æ ‡å¹ºå€¼(_pu)å’Œå®é™…åŠŸç‡(_MW)")
print("  - è¯„ä¼°æŒ‡æ ‡é‡‡ç”¨ç™¾åˆ†æ¯”å½¢å¼ (%)")
print("  - Cap = 50 MW (æ€»è£…æœºå®¹é‡)")
print("  - æµ‹è¯•é›†: 365å¤©æ•°æ®")
print("  - è®­ç»ƒé›†: å¸¸è§„åœºæ™¯æ•°æ®")
print("="*70)

# Display statistics
print("\næ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
print(stats_df.to_string(index=False))
print("="*70)
